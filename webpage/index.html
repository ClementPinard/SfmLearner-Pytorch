<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">
  <title> project </title>
  <link rel="shortcut icon" href="data/img/llama.png" type="image/png"/>
  <!-- Bootstrap core CSS -->
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/project.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<!-- Navigation bar on the side -->
<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary" id="sideNav">
    <a class="navbar-brand" href="#page-top">
      <span class="d-block d-lg-none">ingridn</span>
    </a>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="#intro">Introduction</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#approach">Approach</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#experiments">Experiments</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#results">Results</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#discussion">Discussion</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#ref">References</a>
        </li>
      </ul>
    </div>
  </nav>

  <!-- Introduction -->
  
  <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="intro">
    <div class="w-100">
      <h3 class="mb-3">16-726: <span class="text-primary">Learning-based Image Synthesis</span></h3>
      <h3 class="mb-3">Final Project: <span class="text-primary"> Semantics-Driven View Synthesis</span></h3>
      <h5> <span class="text-primary"> Ingrid Navarro-Anaya </span> (ingridn), <span class="text-primary"> Suann Chi </span> (suannc) </h5>

        <hr class="mb-5"/>  
        <div class="container">
          <h3 class="mb-3">INTRODUCTION</h3>
          <p class="lead mb-3">
          View synthesis is an extremely useful methodology through which a machine agent can autonomously navigate the 3D world.
            It is easy for a human being to draw upon their millions of experiences in different 3D environments to navigate.
            This same task is much harder for a machine. Therefore, the generation or prediction of ego motion sequences based on a
            3D input scene is a hot research topic.
          </p>

          <p class="lead mb-3">
            Previous research in view synthesis has shown that models can be trained to generate reasonable ego motion outputs given
            video inputs from car dashboards (i.e. the KITTI dataset [7])[6]. In this study, a pose and explainability network in
            conjunction with a depth network was used in order to generate a 3D video prediction of the dashboard video.
            Other research employs semantics-driven unsupervised learning in order to train their ego-motion estimation models [8].
          </p>

          <p class="lead mb-3">
            Our project, <b>SemSynSin</b> (<b>Sem</b>antics-driven View<b>Syn</b>thesis from a <b>Sin</b>gle Image), builds on
            these ideas in order to generate ego motion videos from indoor scenes.
          </p>

          <p class="lead mb-3"> 
          The sections that follow are organized as follows: 
          <ul>
            <li class="mb-3">
              In the <code>Approach</code> section, we discuss the approach we take in order to generate indoor ego motion
              videos. For this project, indoor scenes from the MatterPort3D dataset were used. Additionally, trajectory
              information from the Vision-Language Navigation dataset was used.
            </li>
            <li class="mb-3">
              In the <code>Experiments</code> section, we describe the different conditions under which we test our SemSynSin
              model.
            </li>
            <li class="mb-3">
              In the <code>Results</code> section, we show the results generated by the SemSynSin model under the different
              experimentation conditions.
            </li>
            <li class="mb-3">
              In the <code>Discussion</code> section, the results of SemSynSin are discussed, as well as potential avenues of
              future research.
            </li>
          </ul> 
        </div>
      </div>
    </section>

    <hr>

    <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="approach">
      <div class="container">
        <h3 class="mb-3">APPROACH</h3>
        
        <p class="lead mb-3"> 
          In this project, we are interested in learning the 3D structure of 
          complex indoor environments via view-synthesis. View synthesis allows generating 
          images of a scene from different viewpoints. This is a highly 
          challenging task as it requires developing algorithms capable of 
          understanding the nature of 3D environments, <i>e.g.,</i> the semantic
          information in a scene, the relationship between objects in a scene, and
          the layout of environments and occlusions. 
        </p>

        <p class="lead mb-3"> 
          As mentioned above, we build on prior work which focused on learning 
          <strong>Structure-from-Motion</strong> (SfM) from single-view images 
          in outdoor environments.
          We first asses the model's performance on complex indoor environments, 
          and then explore methods for improving the results. Particularily, we
          are interested in explicitly incorporating semantic knowledge since
          it is crucial for scene understanding. 
        </p>

        <p class="lead mb-3"> 
          In the following sub-sections, we further describe the procedure 
          followed in this project. Specifically, in <strong>Part 1</strong>, we 
          described our methodology for obtaining the training data. Then, in 
          <strong>Part 2</strong>, we describe the model and the loss functions 
          we used. 
        </p>

        <h4 class="mb-3 text-primary">Part 1: The Dataset</h4>
        
        <p class="lead mb-3"> 
        We use the 
        <a href="https://niessner.github.io/Matterport/">Matterport3D </a> (MP3D) 
        dataset for our project and the 
        <a href="https://aihabitat.org">Habitat </a> simulation environment to 
        generate egocentric trajectories for training, validation and testing. 

        This section describes in more detail the procedure followed to generate 
        said dataset. 
        </p>
        
        <p class="lead mb-3" > 
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/gt_depth_example.gif" class="img_item_2">
            <img src="data/project/gt_rgb_example.gif" class="img_item_2">
            <img src="data/project/gt_semantic_example.gif" class="img_item_2">
            <figure>
              <strong>Figure 1. Matterport3D trajectory obtained through the 
                Habitat simulation environment. 
              </strong>
          </figure>
          </div>
        </p>

        <h5 class="mb-3 text-primary">1.1: The Scenes</h5>
        
        <p class="lead mb-3"> 
        <strong> Matterport3D </strong> (MP3D) [1] is a large-scale dataset 
        introduced in 2017, featuring 
        over 10k images of 90 different building-scale indoor scenes. The dataset 
        provides annotations with surface reconstructions, camera poses, 
        color and depth images, as well as semantic segmentation images. For our 
        project, we use a different version of this dataset which can be obtained 
        through the Habitat Simulation environment described in 
        <strong> Section 1.2</strong>. It is important to note that one of the 
        major differences between this version of the dataset and the original
        one is that in the former, images have lower resolution and quality. As 
        it can be observed in <strong>Figure 1</strong>, the images exhibit 
        visual artifacts, making the task of 3D learning more challenging. 
        </p>
        
        <p class="lead mb-3">
        As such, this particular version of the dataset
        has been generally used for training Embodied agents in various multi-modal
        navigation tasks [2, 3, 4]. We explore this version of the dataset 
        since we are interested in equipping Embodied agents with 3D learning and 
        understanding skills within this simulation platform. 
        </p>
        
        <h5 class="mb-3 text-primary">1.2: Simulation and Trajectories</h5>
        
        <p class="lead mb-3"> 
        In order to generate the data for training, validation and testing, we 
        used the Vision-and-Language (VLN) dataset presented in [2]. This dataset 
        consists of instructions provided in natural language that describe 
        a particular path to follow in a MP3D indoor environment. These 
        instructions correspond with a trajectory in the environment which 
        can be obtained by using a Shortest-Path-Follower (SPF) from the start and goal 
        locations associated with the instruction. For this project, we are not 
        interested in the language instructions. Thus, we do not provide details 
        on how this dataset has been used to train instruction-following agents. 
        However, we leverage the visual trajectories associated to such instructions
        for creating our dataset. 
        </p>

        <p class="lead mb-3"> 
        The VLN dataset described above was designed for the <strong>Habitat</strong>
        [5] simulation platform. Thus, we use [5] to collect 
        the data for training. Briefly, <strong>Habitat</strong> is a highly efficient 
        and flexible platform intended for 
        embodied research. It allows researchers to easily design and configure
        agents and sensors, as well as AI algorithms for a diverse set of 
        navigation tasks [2, 3, 4]. 
        </p>

        <p class="lead mb-3"> 
        Specifically, we use a SPF, as described above, to get the sensor information 
        from the simulator based on the VLN dataset. We specifically, extract 
        RGB, depth and color images for each trajectory, as well as, relative 
        pose information. The SPF uses has an action space consisting 
        of four possible actions: <code>MOVE_FORWARD 0.25m</code>, 
        <code>TURN_LEFT 15deg</code>, <code>TURN_RIGHT 15deg</code>, 
        and <code>STOP</code>. An example of a resulting trajectory is shown 
        in <strong>Figure 1</strong>. 
        </p>        

        <p class="lead mb-3"> 
          Table 1. shows statistical information about the dataset we 
          obtained. 
        </p>  

        <div class="lead mb-3">
          <table class="styled-table" align="center">
            <caption>Table 1. Dataset statistics</caption>
            <thead>
                <tr>
                    <th>Data Split</th>
                    <th>Num. Environments</th>
                    <th>Avg. Num. Trajectories per Environment</th>
                    <th>Total Num. Trajectories</th>
                    <th>Avg. Num. Steps per Trajectory</th>
                    <th>Total Num. Steps per Trajectory</th>
                </tr>
            </thead>
            <tbody>
                <tr class="active-row">
                  <td style="text-align:center"><code>Train</code></td>
                  <td style="text-align:center">33</td>
                  <td style="text-align:center">65</td>
                  <td style="text-align:center">2,169</td>
                  <td style="text-align:center">55</td>
                  <td style="text-align:center">119,976</td>
                </tr>
                <tr class="active-row">
                  <td style="text-align:center"><code>Val</code></td>
                  <td style="text-align:center">33</td>
                  <td style="text-align:center">5</td>
                  <td style="text-align:center">142</td>
                  <td style="text-align:center">54</td>
                  <td style="text-align:center">7,750</td>
                </tr>
                <tr class="active-row">
                  <td style="text-align:center"><code>Test</code></td>
                  <td style="text-align:center">11</td>
                  <td style="text-align:center">55</td>
                  <td style="text-align:center">613</td>
                  <td style="text-align:center">54</td>
                  <td style="text-align:center">33,412</td>
                </tr>
            </tbody>
          </table>
        </div>

        <hr>

        <h4 class="mb-5 text-primary">Part 2: Model</h4>

        <p class="lead mb-3"> 
        As mentioned before, we focus on learning the 3D structure of an indoor 
        environment from video sequences. We follow prior work [6], which 
        focuses on 
        learning <strong>Structure-from-Motion</strong> (SfM) in outdoor environments. 
        This model achieves the latter purely from training on unlabeled color 
        images and through a view-synthesis objective function as their main 
        supervisory signal. 
        </p>
        
        <p class="lead mb-3"> 
        In our project, we explore if explicitly incorporating semantic information,
        in the form of masks, enables the model to better understand and learn 
        to model the 3D structure of a given scene. Our model  
        jointly trains two neural networks; one in charge of predicting depth 
        from a single-view image represented both in RGB and in semantic labels, 
        and the other in charge of predicting the pose transformaton between 
        two images. To train the model, we also use a view-synthesis objective 
        for both the color images and the segmentations and a multi-scale smoothness 
        loss. <strong>Section 2.1</strong> and <strong>Section 2.2</strong> 
        provide more details on the model implementation, and 
        <strong>Section 2.3</strong> dives into the details of the objective 
        functions.
        </p>
        
        <h5 class="mb-3 text-primary">2.1. Depth Network</h5>

        <p class="lead mb-3"> 
        The first component of the model is the <strong>Depth Network</strong>, a 
        CNN-based model which 
        takes as input a target image represented in color information and 
        semantic masks and outputs the corresponding depth information. 
        As shown in <strong>Figure 2</strong>, the <strong>Depth Network</strong> is comprised 
        by two encoders, one for each input modality, <i>i.e.,</i>
        color and semantic masks, and one decoder which uses the concatenated 
        embeddings of each of the encoders to predict the corresponding depth. 
        </p>

        <p class="lead mb-3" > 
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/depth_network.png" class="img_item_3x">
            <figure>
              <strong>Figure 2. Depth Prediction Network
              </strong>
            </figure>
          </div>
        </p>

        <h5 class="mb-3 text-primary">2.2. Pose Network</h5>

        <p class="lead mb-3"> 
        The second component of the model is the <strong>Pose Network</strong>, 
        which is also a CNN-based network. 
        This module takes as input a short sequence of <i>N</i> images also 
        represented in color and semantic masks. Here,
        one of the images in the sequence is the target image <i>It</i> and all 
        other images are the sources <i>Is</i>. The model then outputs the pose 
        transformation between all source images and the target image. Like the 
        <strong>Depth Network</strong>, the <strong>Pose Network</strong> is 
        comprised by two encoders, one for each input modality. Then, the 
        final embeddings of each encoder are concatenated together and used to 
        predict the pose transformations between the images. The model is shown 
        in <strong>Figure 3</strong>.
        </p>

        <p class="lead mb-3" > 
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/pose_network.png" class="img_item_2x">
            <figure>
              <strong>Figure 3. Pose Prediction Network
              </strong>
            </figure>
          </div>
        </p>

        <h5 class="mb-3 text-primary">2.3: Objective Functions</h5>
        
        <h6 class="mb-3 text-primary">2.3.1: View Synthesis</h6>

        <p class="lead mb-3"> 
        The main objective function in this project comes from a view synthesis
        task: given one input view of a scene, \(I_t\), the goal is to synthesize
        a new image of the scene from a different camera pose. In [6], the 
        synthesis process is achieved by predicting both the depth information 
        of a target viewpoint, \(D_t\), and the pose transformations between the target 
        view, \(T_{t \rightarrow n}\), where \(n\) is the sub-script of the nearby
        view, \(I_n\). Here, the depth and pose information is learned through 
        the CNN-based modules, which were explained in the previous sections. 
        </p>

        <p class="lead mb-3"> 
        The view-synthesis objective is given by the following equation:
        $$ L_{vs} = \sum_{n} \sum_{p} | I_t(p) - \hat{I}_n(p) | $$ where
        \(p\) is a pixel index, and \(\hat{I}_n\) is a nearby image warped 
        into the target's coordinate frame. To warp the nearby image 
        to the target frame, we can project \(p_t\), a homogeneous coordinate 
        of a pixel in the target image onto the nearby image by following 
        the equation below, 
        $$ p_n \sim K \cdot T_{t \rightarrow n} \cdot D_t(p_t) \cdot K^{-1} \cdot p_t $$
        where \(p_t\) represents the homogeneous coordinates of a pixel in the 
        target image, \(K\) is the intrinsics matrix, \(D_t\) and \(T_{t \rightarrow n}\)
        are the predicted depth and pose, respectively. 
        </p>

        <p class="lead mb-3"> 
        Now, the coordinate from the previous equation correspond to continous 
        values. To obtain the value \(I_n(p_n)\) to represent \(\hat{I}_s(p_t)\), 
        we follow two interpolation methods: 1) bilinear interpolation for 
        color images, which linearly interpolates the top-left, top-right, 
        bottom-left and bottom-right pixel neighbors, and 2) nearest interpolation 
        for the semantic masks, to preserve the original label values.
        </p>

        <p class="lead mb-3"> 
        Thus, in summary, the view-synthesis objective is applied to both the 
        color images and the semantic masks by warping the source image into 
        the target frame using the predicted depth and poses, as well as the 
        corresponding interpolation method. 
        </p>
        
        <h6 class="mb-3 text-primary">2.3.2: Artifact Mask</h6>
        
        <p class="lead mb-3"> 
        Unlike [6], the scenes in our indoor environments are assumed to be 
        static, i.e., there are no dynamic objects at any point in a given 
        sequence. However, existing challenges with the dataset include 1) 
        occluding objects and 2) visual artifacts at certain viewpoints resulting 
        from the low quality reconstructions of the images. 
        </p>

        <p class="lead mb-3"> 
        To deal with this, the <strong>Pose Network</strong> is coupled with 
        an <strong>Artifact Network</strong> which is trained to predict a pixel 
        mask, \(E_n(p)\) that represents whether a pixel contributes
        to modeling the 3D structure of a given environment. This mask is used
        to weigh each pixel coordinate in the view-synthesis loss as, 
        $$ L_{vs} = \sum_{n} \sum_{p} E_n(p) | I_t(p) - \hat{I}_n(p) $$
        To avoid the network to predict an all-zeroes mask, the objective is 
        coupled with a regularization term, \(L_{reg}(E_n)\). 
        </p>

        <h6 class="mb-3 text-primary">2.3.3: Large Spatial Regions </h6>

        <p class="lead mb-3"> 
        The final objective function is used to explicitly allow to propagate gradients 
        from large spatial regions in the image, as opposed to only considering 
        the 4 local neighbors of the pixel, as explained in Section 2.3.1. To
        do this, depth maps are predicted at different scales and the \(L_1\) loss 
        of their second-order gradients is minimized as in [6]. 
        </p>

        <p class="lead mb-3"> 
          The final loss then becomes:
          $$ L = \sum_{l} L_{vs}^{l} + \lambda L_{ms}^{l} + \beta \sum_{n} L_{reg}(E^{l}_n)$$
          here, \(l\) is the index of the image scale, \(L_{ms}\) is the multi-scale
          loss and \(\lambda\) and \(\beta\) are weighing hyper-parameters. 
        </p>

      </div>
    </section>
      
    <hr>

      <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="experiments">
        <div class="container">
          <h3 class="mb-3">EXPERIMENTS</h3>

          <h5 class="mb-3 text-primary">3.1 Training Details</h5>
          
          <p class="lead mb-3">
            Our experiments were conducted in a server equipped with 4 GeForce RTX 2080
            GPUs, each with 10GB of memory, Ubuntu 18.04, Pytorch 1.11 and CUD4 11.4.
            With this setup, each experiment took around 4-5 days to complete.
            For this reason, we were only able to define and train three main
            experiments: 
            <ol>
              <li><strong>RGB-only</strong> In this experiment we simply adapted the model introduced
                in [6] to work on our dataset.
              </li>
              <li><strong>Artifact Mask</strong>
                In this experiment an artifact mask was generated in order to mask out occlusion pixels that were less relevant
                to the prediction of the next frame.
              </li>
              <li><strong>RGB and Semantic Encoded</strong>
                In this experiment both RGB and semantic encoded networks were adapted to the model described in [6] to work
                on our dataset.
              </li>
            </ol>
          </p>

          <p class="lead mb-3">
          Due to time and resource constraints, three experiments were conducted for this project: feeding an RGB encoded network
            into the depth network, feeding RGB and semantic encoded networks into the depth network, and using an occlusion/artifact
            mask in order to remove the "least important" pixels in every frame.
          </p>
          
          <h5 class="mb-3 text-primary">3.2: RGB Encoded Network</h5>
          <p class="lead mb-3">
            As mentioned above, this particular experiment adapted Unsupervised Learning of Depth and Ego Motion's depth network
            to the VLN trajectory and Matterport3D datasets [6]. As described in section 2.3.1 in Approach above, bilinear interpolation
            was applied to the RGB colored input scenes. For larger spatial regions where local bilinear interpolation of pixels
            made less sense, the loss function described in 2.3.3 was applied instead.
          </p>

          <h5 class="mb-3 text-primary">3.3: Artifact Mask</h5>
          <p class="lead mb-3">
            As described in section 2.3.2 of Approach above, one of the experiments conducted involved using an artifact
            mask to detect the least important pixels for predicting the next step of the trajectory. Ideally, this mask would
            remove the pixels least important for predicting the next frame, which should optimize the ego motion video produced
            by the model.
          </p>


          <h5 class="mb-3 text-primary">3.4: Semantic and RGB Encoded Network</h5>
          <p class="lead mb-3">
            The semantic segmentation network is one part of the SemSynSin model that needs supervised learning. We believe
            this addition to the depth network would increase depth map clarity due to the following reasons [8]:
          </p>
          <ol>
              <li>
                Semantics don't need to be labelled on depth prediction and pose estimation network training data.
              </li>
            <li>
              Labels for semantic segmentation is possible with work. In fact, it is necessary by today's technological standards
            as there is no easy way to get high-res and accurate depth maps for dynamic scenes.
              </li>
            <li>
              There already exists many large datasets that are labeled in a semantically segmented way.
              </li>
            <li>
              Segmenting semantics is a supervised task.
              </li>
            <li>
             Previous studies that employ semantic segmentation networks have achieved excellent results.
              </li>
            </ol>
          <p class="lead mb-3">
            Based on these reasons, we have used both the RGB encoded network and the semantic encoded network for a final
            experiment. These were fed into the depth network in the manner described in section 2.1 of Approach.
          </p>

        </div>
      </section>

      <hr>

      <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="results">
        <div class="container">
          <h3 class="mb-3">4. RESULTS</h3>
          
          <p class="lead mb-3"> 
            In this section, we compare the results obtained on each of our 
            proposed experiments. Because the approach is modular both, the  
            <strong>Depth Network</strong> and the <strong>Pose Network</strong>
            can be independently assessed. As such, we designed both 
            qualitative and quantitative experiments for these modules. In the 
            paragraphs that follow, we explain each experiment in more detail 
            and show according results for each experiment.
          </p>

          <h5 class="mb-3 text-primary">4.1 Training curves</h5>
          
          <p class="lead mb-3"> 
            Before diving into the qualitative results, we briefly show and discuss, 
            the trainining curves for each experiment in <strong>Figure 4</strong>. 
            First, we can observe that <strong>Experiment 1</strong> had an 
            underfitting behavior: it converged to a high loss and its validation 
            curve had the highest loss values out of all three experiments. Then, 
            we can see that <strong>Experiment 2</strong> exhibits the lowest
            loss values in both curves which is expected since this experiment 
            makes use of the <strong>Artifact Mask</strong>. However, the model 
            barely improves from its initial loss value. Finally, 
            <strong>Experiment 3</strong> has the highest loss value during 
            training due to the additional semantic loss. Nonetheless, the 
            validation loss achieves lower loss values as compared to 
            <strong>Experiment 1</strong>.
            <div class="lead mb-3 img_container" align="center">
              <img src="data/project/training_loss.png" class="img_item_2x">
              <img src="data/project/validation_loss.png" class="img_item_2x">
              <figure>
                <strong>
                  Figure 4. Training (top) and validation (bottom) curves. 
                </strong>
              </figure>
            </div>
          </p>

          <hr>

          <h5 class="mb-3 text-primary">4.2 Qualitative Tests</h5>

          <h6 class="mb-3 text-primary">4.2.1 Depth Network</h6>

          <p class="lead mb-3"> 
            For this qualitative experiment, we simply ran depth inference on 
            a set of trajectories from the test set. We note that, in contrast 
            to the validation set, the environments in the test set were never 
            seen by the networks. The environments in the validation set are 
            the same as in the train set, only the trajectories are different.       
          </p>

          <p class="lead mb-3" > 
            In <strong>Figure 5</strong> we show depth predictions for 
            <strong>Experiment 1</strong> (left-most)
            <strong>Eperiment 2</strong> (center) and <strong>Experiment 3</strong> (right-most).
            First, analyzing the depth results from <strong>Experiment 1</strong> 
            (1st and 2nd columns), we can observe 
            that only using RGB images on this dataset, the model markedly underfitted
            which is consistent with the curves shown in <strong>Figure 4</strong>.
            Here, regardless of the environment and trajectory, the model always predicts very 
            similar results that look like two bright vertical rectangles to the left and right 
            and a darker rectangle to the center. The model also does not seem to understand 
            depth, as it tends to predict darker spots on the bottom center, whereas it should 
            be the opposite since in these visualizations darker means further away. 
          </p>

          <p class="lead mb-3" > 
            In the second experiment (3rd and 4th columns), which studies the 
            effect of the <strong>Artifact Mask</strong>, 
            it seems that the model better understands depth: it tends to predict 
            ceilings to be further away (darker color) and floors at the bottom-center 
            to be closer (lighter color). Nontheless, depth predictions are 
            generally more blurry than the other two experiments. 
          </p>

          <p class="lead mb-3" >
            Finally, in the 
            third experiment (5th and 6th columns) we observe that with semantic
            information taken into account, the predicted depth better captures 
            the structure of the environments, <i>e.g.,</i>
            features like door frames, stairs, etc., are more evident on the 
            predictions. However, every few frames, 
            the model predicts highly blurry images that follow the same diagonal 
            gradient pattern: brightest a the top-left corner and darkest a the 
            bottom-right corner. 
            We posit that this strange behavior may be due to several reasons, 
            for instance:
          </p>
          <ol>
            <li>
              The model was not explicitly trained to handle occlusions or visual artifacts. 
            </li>
            <li>
             There are repetitive patterns in the semantic masks.
            </li>
            <li>
              The shortest-path-follower algorithm we used to obtain the data tends 
              to navigate close to walls and obstacles. For example, if the agent is 
              navigating very close to an object on the wall, i.e. a bookcase, it would
              show up with some features in RGB. However, this same bookcase 
              would show up as a huge featureless colored blob in the semantic mask.
              We theorize that the gradients that are seen here result from the sharp
              cutoff between this semantic featureless blob and the rest of the scene.
            </li>
          </ol>

          <p class="lead mb-3" > 
            
            <div class="lead mb-3 img_container" align="center">
              <img src="data/project/exp1_val/depth_unseen/ep-262_2azQ1b91cZZ-262-rgb-62.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-262_2azQ1b91cZZ-262-rgb-62.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-262_2azQ1b91cZZ-262-rgb-62.gif" class="img_item">

              <img src="data/project/exp1_val/depth_unseen/ep-412_pLe4wQe7qrG-412-rgb-30.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-412_pLe4wQe7qrG-412-rgb-30.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-412_pLe4wQe7qrG-412-rgb-30.gif" class="img_item">

              <img src="data/project/exp1_val/depth_unseen/ep-670_zsNo4HB9uLZ-670-rgb-57.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-670_zsNo4HB9uLZ-670-rgb-57.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-670_zsNo4HB9uLZ-670-rgb-57.gif" class="img_item">

              <img src="data/project/exp1_val/depth_unseen/ep-694_EU6Fwq7SyZv-694-rgb-45.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-694_EU6Fwq7SyZv-694-rgb-45.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-694_EU6Fwq7SyZv-694-rgb-45.gif" class="img_item">

              <img src="data/project/exp1_val/depth_unseen/ep-850_oLBMNvg9in8-850-rgb-88.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-850_oLBMNvg9in8-850-rgb-88.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-850_oLBMNvg9in8-850-rgb-88.gif" class="img_item">

              <img src="data/project/exp1_val/depth_unseen/ep-1066_oLBMNvg9in8-1066-rgb-47.gif" class="img_item">
              <img src="data/project/exp2_val/depth_unseen/ep-1066_oLBMNvg9in8-1066-rgb-47.gif" class="img_item">
              <img src="data/project/exp3_val/depth_unseen/ep-1066_oLBMNvg9in8-1066-rgb-47.gif" class="img_item">
  
              <figure>
                <strong>Figure 4. Depth predictions. Experiment 1 (1st and 2nd cols),
                  Experiment 2 (3rd and 4th cols) and Experiment 3 (5th and 6th cols).  
                </strong>
            </figure>
            </div>
          </p>
        
        <hr>

        <h6 class="mb-3 text-primary">4.2.2 Pose Network</h6>
        
        <p class="lead mb-3"> 
          For this evaluation, we ran pose inference on the same set of 
          trajectories as with the depth network. Specifically, we provide the 
          newtork with a viewpoint at some time step \(t\) and a viewpoint at 
          time step \(t+1\). Then, we predict the pose transformation between them 
          using either ground truth depth or the predicted depth. Finally, 
          we warp the image at time-step \(t+1\) to the coordinate frame of the 
          image at time-step \(t\) and display the corresponding result. 
        </p>

        <p class="lead mb-3"> 
          We show the resulting warps for <strong>Experiment 1</strong> in 
          <strong>Figure 6</strong>. 
          In each row, the left-most image sequence always show the image at time step \(t\),
          the right-most show the image a time step \(t+1\), and the ones at the center 
          are the warps. Here, we used the corresponding ground truth depth. 
          <strong>Figure 7</strong> shows one example comparing the resulting warps 
          when using ground truth depth (top) vs predicted depth (bottom). 
        </p>

        <p class="lead mb-3" > 
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-262_2azQ1b91cZZ-262-rgb-61.gif" class="img_item_3x">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-412_pLe4wQe7qrG-412-rgb-29.gif" class="img_item_3x">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-670_zsNo4HB9uLZ-670-rgb-56.gif" class="img_item_3x">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-694_EU6Fwq7SyZv-694-rgb-44.gif" class="img_item_3x">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-1066_oLBMNvg9in8-1066-rgb-46.gif" class="img_item_3x">
            <figure>
              <strong>Figure 6. Experiment 1 warps. The left-most 
                sequence corresponds to images at time-step \(t\). The right-most 
                sequence corresponds to images a time-step \(t+1\). The sequence 
                at the center corresponds to the warped images from \(t+1\) to 
                \(t\). 
              </strong>
          </figure>
          </div>
        </p>

        <p class="lead mb-3" > 
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/exp1_val/pose_unseen_gt_depth/ep-850_oLBMNvg9in8-850-rgb-87.gif" class="img_item_3x">
            <img src="data/project/exp1_val/pose_unseen_pred_depth/ep-850_oLBMNvg9in8-850-rgb-87.gif" class="img_item_3x">
            <figure>
              <strong>Figure 7. Experiment 1 warps comparing the use of ground 
                truth depth (top) vs predicted depth (bottom). 
              </strong>
          </figure>
          </div>
        </p>

        <p class="lead mb-3" > 
            
          <div class="lead mb-3 img_container" align="center">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-262_2azQ1b91cZZ-262-rgb-61.gif" class="img_item_3x">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-412_pLe4wQe7qrG-412-rgb-29.gif" class="img_item_3x">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-670_zsNo4HB9uLZ-670-rgb-56.gif" class="img_item_3x">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-694_EU6Fwq7SyZv-694-rgb-44.gif" class="img_item_3x">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-850_oLBMNvg9in8-850-rgb-87.gif" class="img_item_3x">
            <img src="data/project/exp2_val/pose_unseen_gt_depth/ep-1066_oLBMNvg9in8-1066-rgb-46.gif" class="img_item_3x">
            <figure>
              <strong>Figure 4. Warp predictions for experiment 1. The left-most 
                sequence corresponds to images at time-step \(t\). The right-most 
                sequence corresponds to images a time-step \(t+1\). The sequence 
                at the center corresponds to the warped images from \(t+1\) to 
                \(t\). 
              </strong>
          </figure>
          </div>
        </p>

        </div>
      </section>
      
      <hr>

      <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="discussion">
        <div class="container">
          <h3 class="mb-3">DISCUSSION</h3>

        <p class="lead mb-3">
          S
        </p>

        <p class="lead mb-3">
          In short, SemSyn works reasonably well in generation of ego motion based on the indoor scenes and trajectories
          given by the MatterPort3D dataset. We believe that the model would be even more accurate at portraying these
          indoor trajectories if they were given even more training time.
        </p>

        <p class="lead mb-3">
          Some things that we can try going forward include:
        </p>
        <ol>
          <li><strong>3D point loss:</strong>
             3D spatial information is considered for this kind of loss. The depth map for the specified target frame can be
            used to grab 3D coordinates for a specific pixel in the target frame. In a similar method, the 3D coordinates for
            the source pixel associated with that specific target pixel can be obtained by applying a transformation matrix to
            the depth maps. After these two have been obtained, 3D loss can be calculated as the difference between the transformed
            source and target pixel. Lower loss indicates better 3D point loss.
          </li>
          <li><strong>Improve multi-scale loss:</strong>

          </li>
        </ol>

        </div>
      </section>

      <hr>
  
    <section class="project-section p-3 p-lg-5 d-flex align-items-center" id="ref">
      <div class="container">
        <h4 class="mb-3 text-primary">REFERENCES</h4>
        <p class="lead mb-5"> 
          <ol>
            <li>Matterport3D: Learning from RGB-D Data in Indoor Environments 
              [<a href="https://niessner.github.io/Matterport/">link</a>]
            </li>
            <li>Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments
              [<a href="https://arxiv.org/abs/2004.02857">link</a>]</li>
            <li>SoundSpaces: Audio-Visual Navigation in 3D Environments
                [<a href="https://arxiv.org/pdf/1912.11474.pdf">link</a>]</li>
            <li>DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames
                [<a href="https://arxiv.org/abs/1911.00357">link</a>]</li>
            <li>Habitat: A Platform for Embodied AI Research
              [<a href="https://arxiv.org/abs/1904.01201">link</a>]</li>
            <li>Unsupervised Learning of Depth and Ego-Motion from Video
                [<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf">link</a>]</li>
            <li>KITTI Dataset
              [<a href="http://www.cvlibs.net/datasets/kitti/">link</a>]</li>
            <li>Semantics Driven Unsupervised Learning for Monocular Depth and Ego Motion Estimation
              [<a href="https://arxiv.org/abs/2006.04371">link</a>]</li>
          </ol>
        </p>
      </div> 
    </div> 
    </section>
  </div>
</body>
</html>
